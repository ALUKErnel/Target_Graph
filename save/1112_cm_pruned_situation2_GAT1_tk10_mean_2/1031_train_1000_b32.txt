Fine-tuning mBERT with options:
Namespace(D_bn=True, D_layers=2, P_bn=True, P_layers=2, alpha=0.8, att_heads='4', attn_dropout=0.2, batch_size=32, batch_size_target=100, beta_l=0.7, beta_t=0.3, concat_domain=False, concat_dropout=0.2, concat_stance=True, data_dir='./dataset/', device='cuda', dropout=0.2, emb_size=768, gnn_dims='192', hidden_size=768, leaky_alpha=0.2, learning_rate=2e-05, local_rank=0, max_epoch=15, max_seq_len=1000, measurement='cosine similarity', model_save_file='./save/1112_cm_pruned_situation2_GAT1_tk10_mean_2', num_target=0, num_train_lines=0, random_seed=2, sim_threshold=0.5, temperature=0.3, tk=10, tokenized_max_len=120, weight_threshold=0.3)
Done loading datasets.
Done constructing DataLoader. 
Done loading models. 
0th target dataset begin encode
1th target dataset begin encode
2th target dataset begin encode
3th target dataset begin encode
4th target dataset begin encode
5th target dataset begin encode
6th target dataset begin encode
7th target dataset begin encode
8th target dataset begin encode
9th target dataset begin encode
10th target dataset begin encode
11th target dataset begin encode
12th target dataset begin encode
13th target dataset begin encode
14th target dataset begin encode
15th target dataset begin encode
16th target dataset begin encode
17th target dataset begin encode
18th target dataset begin encode
19th target dataset begin encode
20th target dataset begin encode
21th target dataset begin encode
22th target dataset begin encode
23th target dataset begin encode
24th target dataset begin encode
25th target dataset begin encode
26th target dataset begin encode
27th target dataset begin encode
28th target dataset begin encode
29th target dataset begin encode
30th target dataset begin encode
Ending epoch 1
Training Accuracy: 60.1141924959217%
Evaluating on valid set:
favor f1: 69.73684210526316
against f1: 54.0
Accuracy on 126 samples: 63.49206349206349%
f1 on 126 samples: 61.86842105263158
Best f1 has been updated as 0.6186842105263158
Evaluating on test set:
favor f1: 67.54385964912281
against f1: 45.58823529411764
Accuracy on 182 samples: 59.34065934065934%
f1 on 182 samples: 56.566047471620216
Ending epoch 2
Training Accuracy: 73.4910277324633%
Evaluating on valid set:
favor f1: 72.41379310344827
against f1: 76.47058823529412
Accuracy on 126 samples: 74.60317460317461%
f1 on 126 samples: 74.44219066937119
Best f1 has been updated as 0.7444219066937119
Evaluating on test set:
favor f1: 73.56321839080458
against f1: 75.78947368421053
Accuracy on 182 samples: 74.72527472527473%
f1 on 182 samples: 74.67634603750756
Ending epoch 3
Training Accuracy: 82.91190864600327%
Evaluating on valid set:
favor f1: 63.366336633663366
against f1: 75.49668874172187
Accuracy on 126 samples: 70.63492063492063%
f1 on 126 samples: 69.4315126876926
Evaluating on test set:
favor f1: 68.96551724137932
against f1: 79.45205479452056
Accuracy on 182 samples: 75.27472527472527%
f1 on 182 samples: 74.20878601794993
Ending epoch 4
Training Accuracy: 90.88499184339315%
Evaluating on valid set:
favor f1: 66.66666666666667
against f1: 79.4871794871795
Accuracy on 126 samples: 74.60317460317461%
f1 on 126 samples: 73.07692307692308
Evaluating on test set:
favor f1: 63.1578947368421
against f1: 78.78787878787878
Accuracy on 182 samples: 73.07692307692307%
f1 on 182 samples: 70.97288676236045
Ending epoch 5
Training Accuracy: 94.80016313213703%
Evaluating on valid set:
favor f1: 62.365591397849464
against f1: 77.9874213836478
Accuracy on 126 samples: 72.22222222222221%
f1 on 126 samples: 70.17650639074864
Evaluating on test set:
favor f1: 62.31884057971014
against f1: 76.99115044247787
Accuracy on 182 samples: 71.42857142857143%
f1 on 182 samples: 69.65499551109401
Ending epoch 6
Training Accuracy: 97.22675367047309%
Evaluating on valid set:
favor f1: 79.41176470588235
against f1: 75.86206896551725
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.6369168356998
Best f1 has been updated as 0.776369168356998
Evaluating on test set:
favor f1: 71.69811320754718
against f1: 60.526315789473685
Accuracy on 182 samples: 67.03296703296702%
f1 on 182 samples: 66.11221449851044
Ending epoch 7
Training Accuracy: 97.24714518760196%
Evaluating on valid set:
favor f1: 71.84466019417475
against f1: 80.53691275167785
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 76.19078647292629
Evaluating on test set:
favor f1: 67.12328767123287
against f1: 77.98165137614677
Accuracy on 182 samples: 73.62637362637363%
f1 on 182 samples: 72.55246952368984
Ending epoch 8
Training Accuracy: 98.53181076672104%
Evaluating on valid set:
favor f1: 68.0
against f1: 78.94736842105264
Accuracy on 126 samples: 74.60317460317461%
f1 on 126 samples: 73.47368421052632
Evaluating on test set:
favor f1: 67.6056338028169
against f1: 79.27927927927927
Accuracy on 182 samples: 74.72527472527473%
f1 on 182 samples: 73.44245654104809
Ending epoch 9
Training Accuracy: 98.77650897226754%
Evaluating on valid set:
favor f1: 73.94957983193278
against f1: 76.69172932330827
Accuracy on 126 samples: 75.39682539682539%
f1 on 126 samples: 75.32065457762052
Evaluating on test set:
favor f1: 68.8888888888889
against f1: 69.56521739130436
Accuracy on 182 samples: 69.23076923076923%
f1 on 182 samples: 69.22705314009663
Ending epoch 10
Training Accuracy: 98.67455138662316%
Evaluating on valid set:
favor f1: 67.96116504854368
against f1: 77.85234899328859
Accuracy on 126 samples: 73.80952380952381%
f1 on 126 samples: 72.90675702091613
Evaluating on test set:
favor f1: 65.21739130434784
against f1: 78.76106194690266
Accuracy on 182 samples: 73.62637362637363%
f1 on 182 samples: 71.98922662562525
Ending epoch 11
Training Accuracy: 98.71533442088091%
Evaluating on valid set:
favor f1: 72.72727272727273
against f1: 78.87323943661971
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.80025608194623
Evaluating on test set:
favor f1: 75.9493670886076
against f1: 81.55339805825243
Accuracy on 182 samples: 79.12087912087912%
f1 on 182 samples: 78.75138257343002
Ending epoch 12
Training Accuracy: 99.14355628058728%
Evaluating on valid set:
favor f1: 61.85567010309278
against f1: 76.12903225806451
Accuracy on 126 samples: 70.63492063492063%
f1 on 126 samples: 68.99235118057865
Evaluating on test set:
favor f1: 64.78873239436619
against f1: 77.47747747747748
Accuracy on 182 samples: 72.52747252747253%
f1 on 182 samples: 71.13310493592184
Ending epoch 13
Training Accuracy: 99.08238172920065%
Evaluating on valid set:
favor f1: 66.66666666666667
against f1: 77.33333333333333
Accuracy on 126 samples: 73.01587301587301%
f1 on 126 samples: 72.0
Evaluating on test set:
favor f1: 73.68421052631578
against f1: 81.1320754716981
Accuracy on 182 samples: 78.02197802197803%
f1 on 182 samples: 77.40814299900694
Ending epoch 14
Training Accuracy: 98.69494290375204%
Evaluating on valid set:
favor f1: 69.23076923076924
against f1: 78.37837837837837
Accuracy on 126 samples: 74.60317460317461%
f1 on 126 samples: 73.80457380457382
Evaluating on test set:
favor f1: 70.12987012987013
against f1: 78.0952380952381
Accuracy on 182 samples: 74.72527472527473%
f1 on 182 samples: 74.11255411255411
Ending epoch 15
Training Accuracy: 99.06199021207178%
Evaluating on valid set:
favor f1: 76.52173913043477
against f1: 80.2919708029197
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.40685496667724
Best f1 has been updated as 0.7840685496667724
Evaluating on test set:
favor f1: 70.73170731707317
against f1: 76.00000000000001
Accuracy on 182 samples: 73.62637362637363%
f1 on 182 samples: 73.36585365853658
Best valid f1 is 0.7840685496667724
