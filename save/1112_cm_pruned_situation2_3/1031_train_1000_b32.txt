Fine-tuning mBERT with options:
Namespace(D_bn=True, D_layers=2, P_bn=True, P_layers=2, alpha=0.8, att_heads='4,4', attn_dropout=0.2, batch_size=32, batch_size_target=100, beta_l=0.7, beta_t=0.3, concat_domain=False, concat_dropout=0.2, concat_stance=True, data_dir='./dataset/', device='cuda', dropout=0.2, emb_size=768, gnn_dims='192,192', hidden_size=768, leaky_alpha=0.2, learning_rate=2e-05, local_rank=0, max_epoch=15, max_seq_len=1000, measurement='cosine similarity', model_save_file='./save/1112_cm_pruned_situation2_3', num_target=0, num_train_lines=0, random_seed=3, sim_threshold=0.9, temperature=0.3, tk=5, tokenized_max_len=120, weight_threshold=0.3)
Done loading datasets.
Done constructing DataLoader. 
Done loading models. 
0th target dataset begin encode
1th target dataset begin encode
2th target dataset begin encode
3th target dataset begin encode
4th target dataset begin encode
5th target dataset begin encode
6th target dataset begin encode
7th target dataset begin encode
8th target dataset begin encode
9th target dataset begin encode
10th target dataset begin encode
11th target dataset begin encode
12th target dataset begin encode
13th target dataset begin encode
14th target dataset begin encode
15th target dataset begin encode
16th target dataset begin encode
17th target dataset begin encode
18th target dataset begin encode
19th target dataset begin encode
20th target dataset begin encode
21th target dataset begin encode
22th target dataset begin encode
23th target dataset begin encode
24th target dataset begin encode
25th target dataset begin encode
26th target dataset begin encode
27th target dataset begin encode
28th target dataset begin encode
29th target dataset begin encode
30th target dataset begin encode
Ending epoch 1
Training Accuracy: 60.0326264274062%
Evaluating on valid set:
favor f1: 74.3801652892562
against f1: 76.33587786259542
Accuracy on 126 samples: 75.39682539682539%
f1 on 126 samples: 75.35802157592582
Best f1 has been updated as 0.7535802157592582
Evaluating on test set:
favor f1: 72.92817679558011
against f1: 73.224043715847
Accuracy on 182 samples: 73.07692307692307%
f1 on 182 samples: 73.07611025571356
Ending epoch 2
Training Accuracy: 74.77569331158239%
Evaluating on valid set:
favor f1: 76.52173913043477
against f1: 80.2919708029197
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.40685496667724
Best f1 has been updated as 0.7840685496667724
Evaluating on test set:
favor f1: 77.64705882352942
against f1: 80.41237113402062
Accuracy on 182 samples: 79.12087912087912%
f1 on 182 samples: 79.02971497877502
Ending epoch 3
Training Accuracy: 85.01223491027733%
Evaluating on valid set:
favor f1: 72.56637168141593
against f1: 77.6978417266187
Accuracy on 126 samples: 75.39682539682539%
f1 on 126 samples: 75.1321067040173
Evaluating on test set:
favor f1: 73.07692307692307
against f1: 79.8076923076923
Accuracy on 182 samples: 76.92307692307693%
f1 on 182 samples: 76.4423076923077
Ending epoch 4
Training Accuracy: 90.88499184339315%
Evaluating on valid set:
favor f1: 76.52173913043477
against f1: 80.2919708029197
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.40685496667724
Evaluating on test set:
favor f1: 74.4186046511628
against f1: 77.08333333333333
Accuracy on 182 samples: 75.82417582417582%
f1 on 182 samples: 75.75096899224806
Ending epoch 5
Training Accuracy: 95.35073409461664%
Evaluating on valid set:
favor f1: 61.53846153846153
against f1: 78.26086956521739
Accuracy on 126 samples: 72.22222222222221%
f1 on 126 samples: 69.89966555183946
Evaluating on test set:
favor f1: 61.19402985074627
against f1: 77.39130434782608
Accuracy on 182 samples: 71.42857142857143%
f1 on 182 samples: 69.29266709928616
Ending epoch 6
Training Accuracy: 96.37030995106036%
Evaluating on valid set:
favor f1: 78.18181818181819
against f1: 83.09859154929576
Accuracy on 126 samples: 80.95238095238095%
f1 on 126 samples: 80.64020486555698
Best f1 has been updated as 0.8064020486555697
Evaluating on test set:
favor f1: 76.07361963190183
against f1: 80.59701492537313
Accuracy on 182 samples: 78.57142857142857%
f1 on 182 samples: 78.33531727863749
Ending epoch 7
Training Accuracy: 97.92006525285481%
Evaluating on valid set:
favor f1: 72.72727272727273
against f1: 78.87323943661971
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.80025608194623
Evaluating on test set:
favor f1: 74.21383647798743
against f1: 80.0
Accuracy on 182 samples: 77.47252747252747%
f1 on 182 samples: 77.10691823899371
Ending epoch 8
Training Accuracy: 98.20554649265905%
Evaluating on valid set:
favor f1: 71.15384615384616
against f1: 79.72972972972973
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.44178794178794
Evaluating on test set:
favor f1: 69.44444444444444
against f1: 80.0
Accuracy on 182 samples: 75.82417582417582%
f1 on 182 samples: 74.72222222222223
Ending epoch 9
Training Accuracy: 98.47063621533442%
Evaluating on valid set:
favor f1: 77.31092436974791
against f1: 79.69924812030075
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.50508624502433
Evaluating on test set:
favor f1: 76.47058823529413
against f1: 79.38144329896907
Accuracy on 182 samples: 78.02197802197803%
f1 on 182 samples: 77.9260157671316
Ending epoch 10
Training Accuracy: 99.26590538336052%
Evaluating on valid set:
favor f1: 73.39449541284404
against f1: 79.72027972027972
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 76.55738756656187
Evaluating on test set:
favor f1: 75.49668874172185
against f1: 82.62910798122066
Accuracy on 182 samples: 79.67032967032966%
f1 on 182 samples: 79.06289836147124
Ending epoch 11
Training Accuracy: 99.18433931484502%
Evaluating on valid set:
favor f1: 77.19298245614034
against f1: 81.15942028985506
Accuracy on 126 samples: 79.36507936507937%
f1 on 126 samples: 79.1762013729977
Evaluating on test set:
favor f1: 73.54838709677418
against f1: 80.38277511961722
Accuracy on 182 samples: 77.47252747252747%
f1 on 182 samples: 76.9655811081957
Ending epoch 12
Training Accuracy: 98.89885807504078%
Evaluating on valid set:
favor f1: 78.63247863247864
against f1: 81.48148148148148
Accuracy on 126 samples: 80.15873015873017%
f1 on 126 samples: 80.05698005698005
Evaluating on test set:
favor f1: 75.86206896551727
against f1: 77.89473684210526
Accuracy on 182 samples: 76.92307692307693%
f1 on 182 samples: 76.87840290381126
Ending epoch 13
Training Accuracy: 98.85807504078304%
Evaluating on valid set:
favor f1: 70.7070707070707
against f1: 81.04575163398692
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 75.87641117052881
Evaluating on test set:
favor f1: 69.50354609929079
against f1: 80.71748878923768
Accuracy on 182 samples: 76.37362637362637%
f1 on 182 samples: 75.11051744426422
Ending epoch 14
Training Accuracy: 98.85807504078304%
Evaluating on valid set:
favor f1: 71.02803738317756
against f1: 78.6206896551724
Accuracy on 126 samples: 75.39682539682539%
f1 on 126 samples: 74.82436351917498
Evaluating on test set:
favor f1: 73.2919254658385
against f1: 78.81773399014779
Accuracy on 182 samples: 76.37362637362637%
f1 on 182 samples: 76.05482972799315
Ending epoch 15
Training Accuracy: 99.49021207177815%
Evaluating on valid set:
favor f1: 75.47169811320754
against f1: 82.19178082191782
Accuracy on 126 samples: 79.36507936507937%
f1 on 126 samples: 78.83173946756268
Evaluating on test set:
favor f1: 75.8169934640523
against f1: 82.46445497630333
Accuracy on 182 samples: 79.67032967032966%
f1 on 182 samples: 79.14072422017782
Best valid f1 is 0.8064020486555697
