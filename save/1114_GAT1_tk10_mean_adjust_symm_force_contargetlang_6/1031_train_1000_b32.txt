Fine-tuning mBERT with options:
Namespace(D_bn=True, D_layers=2, P_bn=True, P_layers=2, alpha=0.8, att_heads='4', attn_dropout=0.2, batch_size=32, batch_size_target=100, beta_l=0.7, beta_t=0.3, concat_domain=False, concat_dropout=0.2, concat_stance=True, data_dir='./dataset/', device='cuda', dropout=0.2, emb_size=768, gnn_dims='192', hidden_size=768, leaky_alpha=0.2, learning_rate=2e-05, local_rank=0, max_epoch=15, max_seq_len=1000, measurement='cosine similarity', model_save_file='./save/1114_GAT1_tk10_mean_adjust_symm_force_contargetlang_6', num_target=0, num_train_lines=0, random_seed=6, sim_threshold=0.4, temperature=0.3, tk=10, tokenized_max_len=120, weight_threshold=0.3)
Done loading datasets.
Done constructing DataLoader. 
Done loading models. 
0th target dataset begin encode
1th target dataset begin encode
2th target dataset begin encode
3th target dataset begin encode
4th target dataset begin encode
5th target dataset begin encode
6th target dataset begin encode
7th target dataset begin encode
8th target dataset begin encode
9th target dataset begin encode
10th target dataset begin encode
11th target dataset begin encode
12th target dataset begin encode
13th target dataset begin encode
14th target dataset begin encode
15th target dataset begin encode
16th target dataset begin encode
17th target dataset begin encode
18th target dataset begin encode
19th target dataset begin encode
20th target dataset begin encode
21th target dataset begin encode
22th target dataset begin encode
23th target dataset begin encode
24th target dataset begin encode
25th target dataset begin encode
26th target dataset begin encode
27th target dataset begin encode
28th target dataset begin encode
29th target dataset begin encode
30th target dataset begin encode
Ending epoch 1
Training Accuracy: 57.68760195758564%
Evaluating on valid set:
favor f1: 56.25
against f1: 73.07692307692307
Accuracy on 126 samples: 66.66666666666666%
f1 on 126 samples: 64.66346153846155
Best f1 has been updated as 0.6466346153846154
Evaluating on test set:
favor f1: 63.1578947368421
against f1: 73.58490566037736
Accuracy on 182 samples: 69.23076923076923%
f1 on 182 samples: 68.37140019860973
Ending epoch 2
Training Accuracy: 73.53181076672104%
Evaluating on valid set:
favor f1: 75.63025210084035
against f1: 78.19548872180451
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 76.91287041132242
Best f1 has been updated as 0.7691287041132242
Evaluating on test set:
favor f1: 77.52808988764045
against f1: 78.49462365591397
Accuracy on 182 samples: 78.02197802197803%
f1 on 182 samples: 78.01135677177722
Ending epoch 3
Training Accuracy: 82.09624796084829%
Evaluating on valid set:
favor f1: 74.07407407407408
against f1: 80.55555555555556
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.31481481481481
Best f1 has been updated as 0.7731481481481481
Evaluating on test set:
favor f1: 74.68354430379746
against f1: 80.58252427184465
Accuracy on 182 samples: 78.02197802197803%
f1 on 182 samples: 77.63303428782106
Ending epoch 4
Training Accuracy: 90.39559543230017%
Evaluating on valid set:
favor f1: 72.72727272727273
against f1: 78.87323943661971
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.80025608194623
Evaluating on test set:
favor f1: 71.3375796178344
against f1: 78.26086956521738
Accuracy on 182 samples: 75.27472527472527%
f1 on 182 samples: 74.79922459152588
Ending epoch 5
Training Accuracy: 94.69820554649266%
Evaluating on valid set:
favor f1: 79.03225806451613
against f1: 79.6875
Accuracy on 126 samples: 79.36507936507937%
f1 on 126 samples: 79.35987903225806
Best f1 has been updated as 0.7935987903225806
Evaluating on test set:
favor f1: 71.20418848167539
against f1: 68.20809248554913
Accuracy on 182 samples: 69.78021978021978%
f1 on 182 samples: 69.70614048361226
Ending epoch 6
Training Accuracy: 96.06443719412724%
Evaluating on valid set:
favor f1: 71.69811320754717
against f1: 79.45205479452055
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.57508400103386
Evaluating on test set:
favor f1: 70.45454545454545
against f1: 72.3404255319149
Accuracy on 182 samples: 71.42857142857143%
f1 on 182 samples: 71.39748549323018
Ending epoch 7
Training Accuracy: 97.89967373572594%
Evaluating on valid set:
favor f1: 69.90291262135922
against f1: 79.19463087248323
Accuracy on 126 samples: 75.39682539682539%
f1 on 126 samples: 74.54877174692123
Evaluating on test set:
favor f1: 73.54838709677418
against f1: 80.38277511961722
Accuracy on 182 samples: 77.47252747252747%
f1 on 182 samples: 76.9655811081957
Ending epoch 8
Training Accuracy: 98.34828711256118%
Evaluating on valid set:
favor f1: 75.67567567567568
against f1: 80.85106382978724
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.26336975273146
Evaluating on test set:
favor f1: 71.8562874251497
against f1: 76.14213197969544
Accuracy on 182 samples: 74.17582417582418%
f1 on 182 samples: 73.99920970242258
Ending epoch 9
Training Accuracy: 99.06199021207178%
Evaluating on valid set:
favor f1: 61.85567010309278
against f1: 76.12903225806451
Accuracy on 126 samples: 70.63492063492063%
f1 on 126 samples: 68.99235118057865
Evaluating on test set:
favor f1: 57.14285714285714
against f1: 75.32467532467534
Accuracy on 182 samples: 68.68131868131869%
f1 on 182 samples: 66.23376623376625
Ending epoch 10
Training Accuracy: 98.36867862969005%
Evaluating on valid set:
favor f1: 73.21428571428574
against f1: 78.57142857142858
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.89285714285717
Evaluating on test set:
favor f1: 71.76470588235294
against f1: 75.2577319587629
Accuracy on 182 samples: 73.62637362637363%
f1 on 182 samples: 73.51121892055792
Ending epoch 11
Training Accuracy: 98.83768352365416%
Evaluating on valid set:
favor f1: 73.87387387387386
against f1: 79.43262411347517
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 76.65324899367451
Evaluating on test set:
favor f1: 71.67630057803468
against f1: 74.3455497382199
Accuracy on 182 samples: 73.07692307692307%
f1 on 182 samples: 73.01092515812728
Ending epoch 12
Training Accuracy: 98.53181076672104%
Evaluating on valid set:
favor f1: 71.69811320754717
against f1: 79.45205479452055
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.57508400103386
Evaluating on test set:
favor f1: 68.67469879518073
against f1: 73.73737373737373
Accuracy on 182 samples: 71.42857142857143%
f1 on 182 samples: 71.20603626627724
Ending epoch 13
Training Accuracy: 98.67455138662316%
Evaluating on valid set:
favor f1: 58.94736842105264
against f1: 75.1592356687898
Accuracy on 126 samples: 69.04761904761905%
f1 on 126 samples: 67.05330204492121
Evaluating on test set:
favor f1: 60.150375939849624
against f1: 77.05627705627705
Accuracy on 182 samples: 70.87912087912088%
f1 on 182 samples: 68.60332649806334
Ending epoch 14
Training Accuracy: 99.1231647634584%
Evaluating on valid set:
favor f1: 64.70588235294117
against f1: 75.99999999999999
Accuracy on 126 samples: 71.42857142857143%
f1 on 126 samples: 70.35294117647058
Evaluating on test set:
favor f1: 63.309352517985616
against f1: 77.33333333333333
Accuracy on 182 samples: 71.97802197802197%
f1 on 182 samples: 70.32134292565948
Ending epoch 15
Training Accuracy: 99.0415986949429%
Evaluating on valid set:
favor f1: 72.72727272727273
against f1: 78.87323943661971
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.80025608194623
Evaluating on test set:
favor f1: 73.05389221556887
against f1: 77.15736040609137
Accuracy on 182 samples: 75.27472527472527%
f1 on 182 samples: 75.1056263108301
Best valid f1 is 0.7935987903225806
