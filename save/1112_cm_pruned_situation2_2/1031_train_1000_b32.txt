Fine-tuning mBERT with options:
Namespace(D_bn=True, D_layers=2, P_bn=True, P_layers=2, alpha=0.8, att_heads='4,4', attn_dropout=0.2, batch_size=32, batch_size_target=100, beta_l=0.7, beta_t=0.3, concat_domain=False, concat_dropout=0.2, concat_stance=True, data_dir='./dataset/', device='cuda', dropout=0.2, emb_size=768, gnn_dims='192,192', hidden_size=768, leaky_alpha=0.2, learning_rate=2e-05, local_rank=0, max_epoch=15, max_seq_len=1000, measurement='cosine similarity', model_save_file='./save/1112_cm_pruned_situation2_2', num_target=0, num_train_lines=0, random_seed=2, sim_threshold=0.9, temperature=0.3, tk=5, tokenized_max_len=120, weight_threshold=0.3)
Done loading datasets.
Done constructing DataLoader. 
Done loading models. 
0th target dataset begin encode
1th target dataset begin encode
2th target dataset begin encode
3th target dataset begin encode
4th target dataset begin encode
5th target dataset begin encode
6th target dataset begin encode
7th target dataset begin encode
8th target dataset begin encode
9th target dataset begin encode
10th target dataset begin encode
11th target dataset begin encode
12th target dataset begin encode
13th target dataset begin encode
14th target dataset begin encode
15th target dataset begin encode
16th target dataset begin encode
17th target dataset begin encode
18th target dataset begin encode
19th target dataset begin encode
20th target dataset begin encode
21th target dataset begin encode
22th target dataset begin encode
23th target dataset begin encode
24th target dataset begin encode
25th target dataset begin encode
26th target dataset begin encode
27th target dataset begin encode
28th target dataset begin encode
29th target dataset begin encode
30th target dataset begin encode
Ending epoch 1
Training Accuracy: 58.340130505709624%
Evaluating on valid set:
favor f1: 75.71428571428572
against f1: 69.64285714285712
Accuracy on 126 samples: 73.01587301587301%
f1 on 126 samples: 72.67857142857143
Best f1 has been updated as 0.7267857142857143
Evaluating on test set:
favor f1: 75.49019607843137
against f1: 68.75
Accuracy on 182 samples: 72.52747252747253%
f1 on 182 samples: 72.12009803921569
Ending epoch 2
Training Accuracy: 72.96084828711255%
Evaluating on valid set:
favor f1: 70.68965517241381
against f1: 75.0
Accuracy on 126 samples: 73.01587301587301%
f1 on 126 samples: 72.84482758620689
Best f1 has been updated as 0.728448275862069
Evaluating on test set:
favor f1: 75.55555555555557
against f1: 76.08695652173913
Accuracy on 182 samples: 75.82417582417582%
f1 on 182 samples: 75.82125603864735
Ending epoch 3
Training Accuracy: 82.97308319738988%
Evaluating on valid set:
favor f1: 78.04878048780488
against f1: 79.06976744186048
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.55927396483267
Best f1 has been updated as 0.7855927396483268
Evaluating on test set:
favor f1: 75.97765363128491
against f1: 76.75675675675674
Accuracy on 182 samples: 76.37362637362637%
f1 on 182 samples: 76.36720519402083
Ending epoch 4
Training Accuracy: 91.45595432300163%
Evaluating on valid set:
favor f1: 72.38095238095238
against f1: 80.27210884353742
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 76.32653061224491
Evaluating on test set:
favor f1: 65.77181208053692
against f1: 76.27906976744187
Accuracy on 182 samples: 71.97802197802197%
f1 on 182 samples: 71.0254409239894
Ending epoch 5
Training Accuracy: 95.69738988580751%
Evaluating on valid set:
favor f1: 75.63025210084035
against f1: 78.19548872180451
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 76.91287041132242
Evaluating on test set:
favor f1: 74.15730337078652
against f1: 75.26881720430109
Accuracy on 182 samples: 74.72527472527473%
f1 on 182 samples: 74.71306028754381
Ending epoch 6
Training Accuracy: 96.98205546492659%
Evaluating on valid set:
favor f1: 72.22222222222221
against f1: 79.16666666666667
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.69444444444444
Evaluating on test set:
favor f1: 71.51515151515152
against f1: 76.3819095477387
Accuracy on 182 samples: 74.17582417582418%
f1 on 182 samples: 73.94853053144512
Ending epoch 7
Training Accuracy: 97.67536704730831%
Evaluating on valid set:
favor f1: 71.42857142857143
against f1: 77.14285714285715
Accuracy on 126 samples: 74.60317460317461%
f1 on 126 samples: 74.28571428571429
Evaluating on test set:
favor f1: 70.11494252873563
against f1: 72.63157894736842
Accuracy on 182 samples: 71.42857142857143%
f1 on 182 samples: 71.37326073805202
Ending epoch 8
Training Accuracy: 98.47063621533442%
Evaluating on valid set:
favor f1: 69.30693069306932
against f1: 79.47019867549669
Accuracy on 126 samples: 75.39682539682539%
f1 on 126 samples: 74.388564684283
Evaluating on test set:
favor f1: 71.523178807947
against f1: 79.81220657276994
Accuracy on 182 samples: 76.37362637362637%
f1 on 182 samples: 75.66769269035846
Ending epoch 9
Training Accuracy: 98.55220228384992%
Evaluating on valid set:
favor f1: 67.3469387755102
against f1: 79.22077922077922
Accuracy on 126 samples: 74.60317460317461%
f1 on 126 samples: 73.28385899814471
Evaluating on test set:
favor f1: 67.11409395973155
against f1: 77.20930232558139
Accuracy on 182 samples: 73.07692307692307%
f1 on 182 samples: 72.16169814265648
Ending epoch 10
Training Accuracy: 98.8784665579119%
Evaluating on valid set:
favor f1: 77.19298245614034
against f1: 81.15942028985506
Accuracy on 126 samples: 79.36507936507937%
f1 on 126 samples: 79.1762013729977
Best f1 has been updated as 0.791762013729977
Evaluating on test set:
favor f1: 71.42857142857143
against f1: 75.51020408163266
Accuracy on 182 samples: 73.62637362637363%
f1 on 182 samples: 73.46938775510205
Ending epoch 11
Training Accuracy: 98.53181076672104%
Evaluating on valid set:
favor f1: 78.63247863247864
against f1: 81.48148148148148
Accuracy on 126 samples: 80.15873015873017%
f1 on 126 samples: 80.05698005698005
Best f1 has been updated as 0.8005698005698005
Evaluating on test set:
favor f1: 72.31638418079096
against f1: 73.79679144385027
Accuracy on 182 samples: 73.07692307692307%
f1 on 182 samples: 73.05658781232061
Ending epoch 12
Training Accuracy: 99.06199021207178%
Evaluating on valid set:
favor f1: 74.54545454545455
against f1: 80.28169014084507
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.41357234314981
Evaluating on test set:
favor f1: 71.51515151515152
against f1: 76.3819095477387
Accuracy on 182 samples: 74.17582417582418%
f1 on 182 samples: 73.94853053144512
Ending epoch 13
Training Accuracy: 99.22512234910278%
Evaluating on valid set:
favor f1: 77.96610169491525
against f1: 80.59701492537313
Accuracy on 126 samples: 79.36507936507937%
f1 on 126 samples: 79.28155831014419
Evaluating on test set:
favor f1: 71.76470588235294
against f1: 75.2577319587629
Accuracy on 182 samples: 73.62637362637363%
f1 on 182 samples: 73.51121892055792
Ending epoch 14
Training Accuracy: 99.26590538336052%
Evaluating on valid set:
favor f1: 78.94736842105264
against f1: 82.6086956521739
Accuracy on 126 samples: 80.95238095238095%
f1 on 126 samples: 80.77803203661328
Best f1 has been updated as 0.8077803203661328
Evaluating on test set:
favor f1: 73.8095238095238
against f1: 77.55102040816327
Accuracy on 182 samples: 75.82417582417582%
f1 on 182 samples: 75.68027210884352
Ending epoch 15
Training Accuracy: 98.93964110929853%
Evaluating on valid set:
favor f1: 73.77049180327869
against f1: 75.38461538461539
Accuracy on 126 samples: 74.60317460317461%
f1 on 126 samples: 74.57755359394704
Evaluating on test set:
favor f1: 75.67567567567566
against f1: 74.86033519553071
Accuracy on 182 samples: 75.27472527472527%
f1 on 182 samples: 75.26800543560319
Best valid f1 is 0.8077803203661328
