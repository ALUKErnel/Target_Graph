Fine-tuning mBERT with options:
Namespace(D_bn=True, D_layers=2, P_bn=True, P_layers=2, alpha=0.8, att_heads='4', attn_dropout=0.2, batch_size=32, batch_size_target=100, beta_l=0.7, beta_t=0.3, concat_domain=False, concat_dropout=0.2, concat_stance=True, data_dir='./dataset/', device='cuda', dropout=0.2, emb_size=768, gnn_dims='192', hidden_size=768, leaky_alpha=0.2, learning_rate=2e-05, local_rank=0, max_epoch=15, max_seq_len=1000, measurement='cosine similarity', model_save_file='./save/1112_cm_pruned_situation2_GAT1_tk10_mean_1', num_target=0, num_train_lines=0, random_seed=1, sim_threshold=0.5, temperature=0.3, tk=10, tokenized_max_len=120, weight_threshold=0.3)
Done loading datasets.
Done constructing DataLoader. 
Done loading models. 
0th target dataset begin encode
1th target dataset begin encode
2th target dataset begin encode
3th target dataset begin encode
4th target dataset begin encode
5th target dataset begin encode
6th target dataset begin encode
7th target dataset begin encode
8th target dataset begin encode
9th target dataset begin encode
10th target dataset begin encode
11th target dataset begin encode
12th target dataset begin encode
13th target dataset begin encode
14th target dataset begin encode
15th target dataset begin encode
16th target dataset begin encode
17th target dataset begin encode
18th target dataset begin encode
19th target dataset begin encode
20th target dataset begin encode
21th target dataset begin encode
22th target dataset begin encode
23th target dataset begin encode
24th target dataset begin encode
25th target dataset begin encode
26th target dataset begin encode
27th target dataset begin encode
28th target dataset begin encode
29th target dataset begin encode
30th target dataset begin encode
Ending epoch 1
Training Accuracy: 60.31810766721044%
Evaluating on valid set:
favor f1: 69.30693069306932
against f1: 79.47019867549669
Accuracy on 126 samples: 75.39682539682539%
f1 on 126 samples: 74.388564684283
Best f1 has been updated as 0.74388564684283
Evaluating on test set:
favor f1: 70.96774193548386
against f1: 78.46889952153109
Accuracy on 182 samples: 75.27472527472527%
f1 on 182 samples: 74.71832072850748
Ending epoch 2
Training Accuracy: 74.836867862969%
Evaluating on valid set:
favor f1: 74.8091603053435
against f1: 72.72727272727273
Accuracy on 126 samples: 73.80952380952381%
f1 on 126 samples: 73.76821651630812
Evaluating on test set:
favor f1: 73.29842931937172
against f1: 70.52023121387283
Accuracy on 182 samples: 71.97802197802197%
f1 on 182 samples: 71.90933026662228
Ending epoch 3
Training Accuracy: 83.01386623164764%
Evaluating on valid set:
favor f1: 77.41935483870968
against f1: 78.12500000000001
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.77217741935485
Best f1 has been updated as 0.7777217741935485
Evaluating on test set:
favor f1: 75.67567567567566
against f1: 74.86033519553071
Accuracy on 182 samples: 75.27472527472527%
f1 on 182 samples: 75.26800543560319
Ending epoch 4
Training Accuracy: 90.9257748776509%
Evaluating on valid set:
favor f1: 82.17054263565892
against f1: 81.30081300813009
Accuracy on 126 samples: 81.74603174603175%
f1 on 126 samples: 81.7356778218945
Best f1 has been updated as 0.817356778218945
Evaluating on test set:
favor f1: 76.64670658682634
against f1: 80.20304568527919
Accuracy on 182 samples: 78.57142857142857%
f1 on 182 samples: 78.42487613605277
Ending epoch 5
Training Accuracy: 95.26916802610114%
Evaluating on valid set:
favor f1: 71.55963302752293
against f1: 78.32167832167832
Accuracy on 126 samples: 75.39682539682539%
f1 on 126 samples: 74.94065567460062
Evaluating on test set:
favor f1: 68.91891891891892
against f1: 78.7037037037037
Accuracy on 182 samples: 74.72527472527473%
f1 on 182 samples: 73.81131131131131
Ending epoch 6
Training Accuracy: 96.45187601957586%
Evaluating on valid set:
favor f1: 80.32786885245902
against f1: 81.53846153846153
Accuracy on 126 samples: 80.95238095238095%
f1 on 126 samples: 80.93316519546028
Evaluating on test set:
favor f1: 75.7396449704142
against f1: 78.97435897435898
Accuracy on 182 samples: 77.47252747252747%
f1 on 182 samples: 77.3570019723866
Ending epoch 7
Training Accuracy: 98.6541598694943%
Evaluating on valid set:
favor f1: 80.99173553719008
against f1: 82.44274809160305
Accuracy on 126 samples: 81.74603174603175%
f1 on 126 samples: 81.71724181439657
Evaluating on test set:
favor f1: 73.98843930635837
against f1: 76.4397905759162
Accuracy on 182 samples: 75.27472527472527%
f1 on 182 samples: 75.2141149411373
Ending epoch 8
Training Accuracy: 98.4910277324633%
Evaluating on valid set:
favor f1: 75.0
against f1: 80.0
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.5
Evaluating on test set:
favor f1: 70.44025157232704
against f1: 77.07317073170732
Accuracy on 182 samples: 74.17582417582418%
f1 on 182 samples: 73.75671115201719
Ending epoch 9
Training Accuracy: 98.42985318107667%
Evaluating on valid set:
favor f1: 72.72727272727273
against f1: 78.87323943661971
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.80025608194623
Evaluating on test set:
favor f1: 67.0967741935484
against f1: 75.59808612440192
Accuracy on 182 samples: 71.97802197802197%
f1 on 182 samples: 71.34743015897516
Ending epoch 10
Training Accuracy: 98.9600326264274%
Evaluating on valid set:
favor f1: 77.68595041322315
against f1: 79.38931297709924
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.5376316951612
Evaluating on test set:
favor f1: 72.83236994219654
against f1: 75.39267015706807
Accuracy on 182 samples: 74.17582417582418%
f1 on 182 samples: 74.11252004963231
Ending epoch 11
Training Accuracy: 98.7357259380098%
Evaluating on valid set:
favor f1: 65.3061224489796
against f1: 77.92207792207793
Accuracy on 126 samples: 73.01587301587301%
f1 on 126 samples: 71.61410018552876
Evaluating on test set:
favor f1: 67.12328767123287
against f1: 77.98165137614677
Accuracy on 182 samples: 73.62637362637363%
f1 on 182 samples: 72.55246952368984
Ending epoch 12
Training Accuracy: 99.18433931484502%
Evaluating on valid set:
favor f1: 68.51851851851852
against f1: 76.38888888888889
Accuracy on 126 samples: 73.01587301587301%
f1 on 126 samples: 72.4537037037037
Evaluating on test set:
favor f1: 73.41772151898735
against f1: 79.6116504854369
Accuracy on 182 samples: 76.92307692307693%
f1 on 182 samples: 76.51468600221213
Ending epoch 13
Training Accuracy: 98.7357259380098%
Evaluating on valid set:
favor f1: 75.43859649122807
against f1: 79.71014492753623
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.57437070938215
Evaluating on test set:
favor f1: 73.49397590361447
against f1: 77.77777777777777
Accuracy on 182 samples: 75.82417582417582%
f1 on 182 samples: 75.63587684069613
Ending epoch 14
Training Accuracy: 99.22512234910278%
Evaluating on valid set:
favor f1: 76.52173913043477
against f1: 80.2919708029197
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.40685496667724
Evaluating on test set:
favor f1: 73.93939393939394
against f1: 78.39195979899498
Accuracy on 182 samples: 76.37362637362637%
f1 on 182 samples: 76.16567686919447
Ending epoch 15
Training Accuracy: 99.46982055464926%
Evaluating on valid set:
favor f1: 74.33628318584071
against f1: 79.13669064748201
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 76.73648691666136
Evaluating on test set:
favor f1: 76.74418604651162
against f1: 79.16666666666666
Accuracy on 182 samples: 78.02197802197803%
f1 on 182 samples: 77.95542635658914
Best valid f1 is 0.817356778218945
Fine-tuning mBERT with options:
Namespace(D_bn=True, D_layers=2, P_bn=True, P_layers=2, alpha=0.7, att_heads='4', attn_dropout=0.2, batch_size=32, batch_size_target=100, beta_l=0.5, beta_t=0.5, concat_domain=False, concat_dropout=0.2, concat_stance=True, data_dir='./dataset/', device='cuda', dropout=0.2, emb_size=768, gnn_dims='192', hidden_size=768, leaky_alpha=0.2, learning_rate=2e-05, local_rank=0, max_epoch=15, max_seq_len=1000, measurement='cosine similarity', model_save_file='./save/1112_cm_pruned_situation2_GAT1_tk10_mean_1', num_target=0, num_train_lines=0, random_seed=1, sim_threshold=0.4, temperature=0.3, tk=10, tokenized_max_len=120, weight_threshold=0.3)
Done loading datasets.
Done constructing DataLoader. 
Done loading models. 
0th target dataset begin encode
1th target dataset begin encode
2th target dataset begin encode
3th target dataset begin encode
4th target dataset begin encode
5th target dataset begin encode
6th target dataset begin encode
7th target dataset begin encode
8th target dataset begin encode
9th target dataset begin encode
10th target dataset begin encode
11th target dataset begin encode
12th target dataset begin encode
13th target dataset begin encode
14th target dataset begin encode
15th target dataset begin encode
16th target dataset begin encode
17th target dataset begin encode
18th target dataset begin encode
19th target dataset begin encode
20th target dataset begin encode
21th target dataset begin encode
22th target dataset begin encode
23th target dataset begin encode
24th target dataset begin encode
25th target dataset begin encode
26th target dataset begin encode
27th target dataset begin encode
28th target dataset begin encode
29th target dataset begin encode
30th target dataset begin encode
Fine-tuning mBERT with options:
Namespace(D_bn=True, D_layers=2, P_bn=True, P_layers=2, alpha=0.7, att_heads='4', attn_dropout=0.2, batch_size=32, batch_size_target=100, beta_l=0.5, beta_t=0.5, concat_domain=False, concat_dropout=0.2, concat_stance=True, data_dir='./dataset/', device='cuda', dropout=0.2, emb_size=768, gnn_dims='192', hidden_size=768, leaky_alpha=0.2, learning_rate=2e-05, local_rank=0, max_epoch=15, max_seq_len=1000, measurement='cosine similarity', model_save_file='./save/1112_cm_pruned_situation2_GAT1_tk10_mean_1', num_target=0, num_train_lines=0, random_seed=1, sim_threshold=0.4, temperature=0.3, tk=10, tokenized_max_len=120, weight_threshold=0.3)
Done loading datasets.
Done constructing DataLoader. 
Done loading models. 
0th target dataset begin encode
1th target dataset begin encode
2th target dataset begin encode
3th target dataset begin encode
4th target dataset begin encode
5th target dataset begin encode
6th target dataset begin encode
7th target dataset begin encode
8th target dataset begin encode
9th target dataset begin encode
10th target dataset begin encode
11th target dataset begin encode
12th target dataset begin encode
13th target dataset begin encode
14th target dataset begin encode
15th target dataset begin encode
16th target dataset begin encode
17th target dataset begin encode
18th target dataset begin encode
19th target dataset begin encode
20th target dataset begin encode
21th target dataset begin encode
22th target dataset begin encode
23th target dataset begin encode
24th target dataset begin encode
25th target dataset begin encode
26th target dataset begin encode
27th target dataset begin encode
28th target dataset begin encode
29th target dataset begin encode
30th target dataset begin encode
Fine-tuning mBERT with options:
Namespace(D_bn=True, D_layers=2, P_bn=True, P_layers=2, alpha=0.7, att_heads='4', attn_dropout=0.2, batch_size=32, batch_size_target=100, beta_l=0.5, beta_t=0.5, concat_domain=False, concat_dropout=0.2, concat_stance=True, data_dir='./dataset/', device='cuda', dropout=0.2, emb_size=768, gnn_dims='192', hidden_size=768, leaky_alpha=0.2, learning_rate=2e-05, local_rank=0, max_epoch=15, max_seq_len=1000, measurement='cosine similarity', model_save_file='./save/1112_cm_pruned_situation2_GAT1_tk10_mean_1', num_target=0, num_train_lines=0, random_seed=1, sim_threshold=0.4, temperature=0.3, tk=10, tokenized_max_len=120, weight_threshold=0.3)
Done loading datasets.
Done constructing DataLoader. 
Done loading models. 
0th target dataset begin encode
1th target dataset begin encode
2th target dataset begin encode
3th target dataset begin encode
4th target dataset begin encode
5th target dataset begin encode
6th target dataset begin encode
7th target dataset begin encode
8th target dataset begin encode
9th target dataset begin encode
10th target dataset begin encode
11th target dataset begin encode
12th target dataset begin encode
13th target dataset begin encode
14th target dataset begin encode
15th target dataset begin encode
16th target dataset begin encode
17th target dataset begin encode
18th target dataset begin encode
19th target dataset begin encode
20th target dataset begin encode
21th target dataset begin encode
22th target dataset begin encode
23th target dataset begin encode
24th target dataset begin encode
25th target dataset begin encode
26th target dataset begin encode
27th target dataset begin encode
28th target dataset begin encode
29th target dataset begin encode
30th target dataset begin encode
Fine-tuning mBERT with options:
Namespace(D_bn=True, D_layers=2, P_bn=True, P_layers=2, alpha=0.7, att_heads='4', attn_dropout=0.2, batch_size=32, batch_size_target=100, beta_l=0.5, beta_t=0.5, concat_domain=False, concat_dropout=0.2, concat_stance=True, data_dir='./dataset/', device='cuda', dropout=0.2, emb_size=768, gnn_dims='192', hidden_size=768, leaky_alpha=0.2, learning_rate=2e-05, local_rank=0, max_epoch=15, max_seq_len=1000, measurement='cosine similarity', model_save_file='./save/1112_cm_pruned_situation2_GAT1_tk10_mean_1', num_target=0, num_train_lines=0, random_seed=1, sim_threshold=0.4, temperature=0.3, tk=10, tokenized_max_len=120, weight_threshold=0.3)
Done loading datasets.
Done constructing DataLoader. 
Done loading models. 
0th target dataset begin encode
1th target dataset begin encode
2th target dataset begin encode
3th target dataset begin encode
4th target dataset begin encode
5th target dataset begin encode
6th target dataset begin encode
7th target dataset begin encode
8th target dataset begin encode
9th target dataset begin encode
10th target dataset begin encode
11th target dataset begin encode
12th target dataset begin encode
13th target dataset begin encode
14th target dataset begin encode
15th target dataset begin encode
16th target dataset begin encode
17th target dataset begin encode
18th target dataset begin encode
19th target dataset begin encode
20th target dataset begin encode
21th target dataset begin encode
22th target dataset begin encode
23th target dataset begin encode
24th target dataset begin encode
25th target dataset begin encode
26th target dataset begin encode
27th target dataset begin encode
28th target dataset begin encode
29th target dataset begin encode
30th target dataset begin encode
