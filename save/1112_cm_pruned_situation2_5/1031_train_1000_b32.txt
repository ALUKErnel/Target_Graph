Fine-tuning mBERT with options:
Namespace(D_bn=True, D_layers=2, P_bn=True, P_layers=2, alpha=0.8, att_heads='4,4', attn_dropout=0.2, batch_size=32, batch_size_target=100, beta_l=0.7, beta_t=0.3, concat_domain=False, concat_dropout=0.2, concat_stance=True, data_dir='./dataset/', device='cuda', dropout=0.2, emb_size=768, gnn_dims='192,192', hidden_size=768, leaky_alpha=0.2, learning_rate=2e-05, local_rank=0, max_epoch=15, max_seq_len=1000, measurement='cosine similarity', model_save_file='./save/1112_cm_pruned_situation2_5', num_target=0, num_train_lines=0, random_seed=5, sim_threshold=0.9, temperature=0.3, tk=5, tokenized_max_len=120, weight_threshold=0.3)
Done loading datasets.
Done constructing DataLoader. 
Done loading models. 
0th target dataset begin encode
1th target dataset begin encode
2th target dataset begin encode
3th target dataset begin encode
4th target dataset begin encode
5th target dataset begin encode
6th target dataset begin encode
7th target dataset begin encode
8th target dataset begin encode
9th target dataset begin encode
10th target dataset begin encode
11th target dataset begin encode
12th target dataset begin encode
13th target dataset begin encode
14th target dataset begin encode
15th target dataset begin encode
16th target dataset begin encode
17th target dataset begin encode
18th target dataset begin encode
19th target dataset begin encode
20th target dataset begin encode
21th target dataset begin encode
22th target dataset begin encode
23th target dataset begin encode
24th target dataset begin encode
25th target dataset begin encode
26th target dataset begin encode
27th target dataset begin encode
28th target dataset begin encode
29th target dataset begin encode
30th target dataset begin encode
Ending epoch 1
Training Accuracy: 61.317292006525285%
Evaluating on valid set:
favor f1: 52.38095238095239
against f1: 76.19047619047619
Accuracy on 126 samples: 68.25396825396825%
f1 on 126 samples: 64.28571428571428
Best f1 has been updated as 0.6428571428571428
Evaluating on test set:
favor f1: 43.69747899159664
against f1: 72.65306122448979
Accuracy on 182 samples: 63.18681318681318%
f1 on 182 samples: 58.17527010804322
Ending epoch 2
Training Accuracy: 73.30750407830342%
Evaluating on valid set:
favor f1: 74.07407407407408
against f1: 80.55555555555556
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.31481481481481
Best f1 has been updated as 0.7731481481481481
Evaluating on test set:
favor f1: 72.15189873417721
against f1: 78.64077669902913
Accuracy on 182 samples: 75.82417582417582%
f1 on 182 samples: 75.39633771660317
Ending epoch 3
Training Accuracy: 82.6468189233279%
Evaluating on valid set:
favor f1: 75.2136752136752
against f1: 78.51851851851852
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 76.86609686609687
Evaluating on test set:
favor f1: 78.21229050279331
against f1: 78.9189189189189
Accuracy on 182 samples: 78.57142857142857%
f1 on 182 samples: 78.5656047108561
Ending epoch 4
Training Accuracy: 90.68107667210441%
Evaluating on valid set:
favor f1: 80.62015503875969
against f1: 79.67479674796748
Accuracy on 126 samples: 80.15873015873017%
f1 on 126 samples: 80.14747589336359
Best f1 has been updated as 0.8014747589336358
Evaluating on test set:
favor f1: 75.70621468926554
against f1: 77.00534759358288
Accuracy on 182 samples: 76.37362637362637%
f1 on 182 samples: 76.3557811414242
Ending epoch 5
Training Accuracy: 94.94290375203916%
Evaluating on valid set:
favor f1: 83.33333333333333
against f1: 84.84848484848484
Accuracy on 126 samples: 84.12698412698413%
f1 on 126 samples: 84.09090909090908
Best f1 has been updated as 0.8409090909090908
Evaluating on test set:
favor f1: 73.86363636363636
against f1: 75.53191489361703
Accuracy on 182 samples: 74.72527472527473%
f1 on 182 samples: 74.6977756286267
Ending epoch 6
Training Accuracy: 97.12479608482872%
Evaluating on valid set:
favor f1: 78.63247863247864
against f1: 81.48148148148148
Accuracy on 126 samples: 80.15873015873017%
f1 on 126 samples: 80.05698005698005
Evaluating on test set:
favor f1: 70.30303030303031
against f1: 75.37688442211056
Accuracy on 182 samples: 73.07692307692307%
f1 on 182 samples: 72.83995736257044
Ending epoch 7
Training Accuracy: 98.06280587275694%
Evaluating on valid set:
favor f1: 60.215053763440864
against f1: 76.72955974842768
Accuracy on 126 samples: 70.63492063492063%
f1 on 126 samples: 68.47230675593427
Evaluating on test set:
favor f1: 61.19402985074627
against f1: 77.39130434782608
Accuracy on 182 samples: 71.42857142857143%
f1 on 182 samples: 69.29266709928616
Ending epoch 8
Training Accuracy: 98.06280587275694%
Evaluating on valid set:
favor f1: 76.1061946902655
against f1: 80.57553956834532
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.34086712930541
Evaluating on test set:
favor f1: 72.0
against f1: 74.07407407407408
Accuracy on 182 samples: 73.07692307692307%
f1 on 182 samples: 73.03703703703704
Ending epoch 9
Training Accuracy: 98.7357259380098%
Evaluating on valid set:
favor f1: 74.07407407407408
against f1: 80.55555555555556
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.31481481481481
Evaluating on test set:
favor f1: 69.56521739130434
against f1: 75.86206896551725
Accuracy on 182 samples: 73.07692307692307%
f1 on 182 samples: 72.7136431784108
Ending epoch 10
Training Accuracy: 98.30750407830342%
Evaluating on valid set:
favor f1: 79.06976744186046
against f1: 78.04878048780488
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.55927396483267
Evaluating on test set:
favor f1: 76.53061224489795
against f1: 72.61904761904762
Accuracy on 182 samples: 74.72527472527473%
f1 on 182 samples: 74.57482993197279
Ending epoch 11
Training Accuracy: 98.85807504078304%
Evaluating on valid set:
favor f1: 77.19298245614034
against f1: 81.15942028985506
Accuracy on 126 samples: 79.36507936507937%
f1 on 126 samples: 79.1762013729977
Evaluating on test set:
favor f1: 73.25581395348838
against f1: 76.04166666666666
Accuracy on 182 samples: 74.72527472527473%
f1 on 182 samples: 74.64874031007753
Ending epoch 12
Training Accuracy: 99.06199021207178%
Evaluating on valid set:
favor f1: 70.7070707070707
against f1: 81.04575163398692
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 75.87641117052881
Evaluating on test set:
favor f1: 63.829787234042556
against f1: 77.13004484304933
Accuracy on 182 samples: 71.97802197802197%
f1 on 182 samples: 70.47991603854594
Ending epoch 13
Training Accuracy: 98.9600326264274%
Evaluating on valid set:
favor f1: 73.07692307692307
against f1: 81.08108108108108
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.07900207900207
Evaluating on test set:
favor f1: 66.66666666666666
against f1: 77.41935483870968
Accuracy on 182 samples: 73.07692307692307%
f1 on 182 samples: 72.04301075268818
Ending epoch 14
Training Accuracy: 98.38907014681892%
Evaluating on valid set:
favor f1: 75.43859649122807
against f1: 79.71014492753623
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.57437070938215
Evaluating on test set:
favor f1: 72.72727272727272
against f1: 77.38693467336684
Accuracy on 182 samples: 75.27472527472527%
f1 on 182 samples: 75.05710370031977
Ending epoch 15
Training Accuracy: 98.9600326264274%
Evaluating on valid set:
favor f1: 77.06422018348624
against f1: 82.5174825174825
Accuracy on 126 samples: 80.15873015873017%
f1 on 126 samples: 79.79085135048439
Evaluating on test set:
favor f1: 71.95121951219512
against f1: 77.0
Accuracy on 182 samples: 74.72527472527473%
f1 on 182 samples: 74.47560975609755
Best valid f1 is 0.8409090909090908
