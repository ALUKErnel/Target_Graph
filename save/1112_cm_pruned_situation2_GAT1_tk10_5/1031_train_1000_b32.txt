Fine-tuning mBERT with options:
Namespace(D_bn=True, D_layers=2, P_bn=True, P_layers=2, alpha=0.8, att_heads='4', attn_dropout=0.2, batch_size=32, batch_size_target=100, beta_l=0.7, beta_t=0.3, concat_domain=False, concat_dropout=0.2, concat_stance=True, data_dir='./dataset/', device='cuda', dropout=0.2, emb_size=768, gnn_dims='192', hidden_size=768, leaky_alpha=0.2, learning_rate=2e-05, local_rank=0, max_epoch=15, max_seq_len=1000, measurement='cosine similarity', model_save_file='./save/1112_cm_pruned_situation2_GAT1_tk10_5', num_target=0, num_train_lines=0, random_seed=5, sim_threshold=0.9, temperature=0.3, tk=10, tokenized_max_len=120, weight_threshold=0.3)
Done loading datasets.
Done constructing DataLoader. 
Done loading models. 
0th target dataset begin encode
1th target dataset begin encode
2th target dataset begin encode
3th target dataset begin encode
4th target dataset begin encode
5th target dataset begin encode
6th target dataset begin encode
7th target dataset begin encode
8th target dataset begin encode
9th target dataset begin encode
10th target dataset begin encode
11th target dataset begin encode
12th target dataset begin encode
13th target dataset begin encode
14th target dataset begin encode
15th target dataset begin encode
16th target dataset begin encode
17th target dataset begin encode
18th target dataset begin encode
19th target dataset begin encode
20th target dataset begin encode
21th target dataset begin encode
22th target dataset begin encode
23th target dataset begin encode
24th target dataset begin encode
25th target dataset begin encode
26th target dataset begin encode
27th target dataset begin encode
28th target dataset begin encode
29th target dataset begin encode
30th target dataset begin encode
Ending epoch 1
Training Accuracy: 58.768352365415986%
Evaluating on valid set:
favor f1: 62.22222222222222
against f1: 79.01234567901234
Accuracy on 126 samples: 73.01587301587301%
f1 on 126 samples: 70.61728395061728
Best f1 has been updated as 0.7061728395061728
Evaluating on test set:
favor f1: 64.74820143884892
against f1: 78.22222222222223
Accuracy on 182 samples: 73.07692307692307%
f1 on 182 samples: 71.48521183053558
Ending epoch 2
Training Accuracy: 72.92006525285481%
Evaluating on valid set:
favor f1: 73.58490566037736
against f1: 80.82191780821918
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.20341173429827
Best f1 has been updated as 0.7720341173429828
Evaluating on test set:
favor f1: 76.72955974842768
against f1: 81.95121951219512
Accuracy on 182 samples: 79.67032967032966%
f1 on 182 samples: 79.3403896303114
Ending epoch 3
Training Accuracy: 82.09624796084829%
Evaluating on valid set:
favor f1: 71.2121212121212
against f1: 68.33333333333333
Accuracy on 126 samples: 69.84126984126983%
f1 on 126 samples: 69.77272727272728
Evaluating on test set:
favor f1: 73.46938775510203
against f1: 69.04761904761905
Accuracy on 182 samples: 71.42857142857143%
f1 on 182 samples: 71.25850340136056
Ending epoch 4
Training Accuracy: 90.00815660685154%
Evaluating on valid set:
favor f1: 76.52173913043477
against f1: 80.2919708029197
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.40685496667724
Best f1 has been updated as 0.7840685496667724
Evaluating on test set:
favor f1: 74.11764705882354
against f1: 77.31958762886599
Accuracy on 182 samples: 75.82417582417582%
f1 on 182 samples: 75.71861734384476
Ending epoch 5
Training Accuracy: 94.41272430668842%
Evaluating on valid set:
favor f1: 76.92307692307693
against f1: 80.0
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.46153846153847
Best f1 has been updated as 0.7846153846153847
Evaluating on test set:
favor f1: 73.25581395348838
against f1: 76.04166666666666
Accuracy on 182 samples: 74.72527472527473%
f1 on 182 samples: 74.64874031007753
Ending epoch 6
Training Accuracy: 96.37030995106036%
Evaluating on valid set:
favor f1: 75.43859649122807
against f1: 79.71014492753623
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.57437070938215
Evaluating on test set:
favor f1: 74.55621301775149
against f1: 77.94871794871796
Accuracy on 182 samples: 76.37362637362637%
f1 on 182 samples: 76.25246548323472
Ending epoch 7
Training Accuracy: 97.38988580750407%
Evaluating on valid set:
favor f1: 74.54545454545455
against f1: 80.28169014084507
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.41357234314981
Evaluating on test set:
favor f1: 73.41772151898735
against f1: 79.6116504854369
Accuracy on 182 samples: 76.92307692307693%
f1 on 182 samples: 76.51468600221213
Ending epoch 8
Training Accuracy: 98.20554649265905%
Evaluating on valid set:
favor f1: 73.07692307692307
against f1: 81.08108108108108
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.07900207900207
Evaluating on test set:
favor f1: 71.89542483660131
against f1: 79.62085308056871
Accuracy on 182 samples: 76.37362637362637%
f1 on 182 samples: 75.75813895858501
Ending epoch 9
Training Accuracy: 98.81729200652529%
Evaluating on valid set:
favor f1: 78.18181818181819
against f1: 83.09859154929576
Accuracy on 126 samples: 80.95238095238095%
f1 on 126 samples: 80.64020486555698
Best f1 has been updated as 0.8064020486555697
Evaluating on test set:
favor f1: 74.99999999999999
against f1: 78.57142857142858
Accuracy on 182 samples: 76.92307692307693%
f1 on 182 samples: 76.78571428571428
Ending epoch 10
Training Accuracy: 98.55220228384992%
Evaluating on valid set:
favor f1: 76.66666666666666
against f1: 78.7878787878788
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.72727272727273
Evaluating on test set:
favor f1: 77.52808988764045
against f1: 78.49462365591397
Accuracy on 182 samples: 78.02197802197803%
f1 on 182 samples: 78.01135677177722
Ending epoch 11
Training Accuracy: 98.89885807504078%
Evaluating on valid set:
favor f1: 71.15384615384616
against f1: 79.72972972972973
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.44178794178794
Evaluating on test set:
favor f1: 76.5432098765432
against f1: 81.18811881188118
Accuracy on 182 samples: 79.12087912087912%
f1 on 182 samples: 78.86566434421219
Ending epoch 12
Training Accuracy: 98.83768352365416%
Evaluating on valid set:
favor f1: 73.58490566037736
against f1: 80.82191780821918
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.20341173429827
Evaluating on test set:
favor f1: 72.83236994219654
against f1: 75.39267015706807
Accuracy on 182 samples: 74.17582417582418%
f1 on 182 samples: 74.11252004963231
Ending epoch 13
Training Accuracy: 98.67455138662316%
Evaluating on valid set:
favor f1: 72.89719626168225
against f1: 80.0
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 76.44859813084113
Evaluating on test set:
favor f1: 72.8395061728395
against f1: 78.2178217821782
Accuracy on 182 samples: 75.82417582417582%
f1 on 182 samples: 75.52866397750886
Ending epoch 14
Training Accuracy: 97.79771615008157%
Evaluating on valid set:
favor f1: 75.67567567567568
against f1: 80.85106382978724
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.26336975273146
Evaluating on test set:
favor f1: 72.5
against f1: 78.43137254901961
Accuracy on 182 samples: 75.82417582417582%
f1 on 182 samples: 75.4656862745098
Ending epoch 15
Training Accuracy: 99.16394779771615%
Evaluating on valid set:
favor f1: 73.39449541284404
against f1: 79.72027972027972
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 76.55738756656187
Evaluating on test set:
favor f1: 73.25581395348838
against f1: 76.04166666666666
Accuracy on 182 samples: 74.72527472527473%
f1 on 182 samples: 74.64874031007753
Best valid f1 is 0.8064020486555697
