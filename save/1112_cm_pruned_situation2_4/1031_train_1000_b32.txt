Fine-tuning mBERT with options:
Namespace(D_bn=True, D_layers=2, P_bn=True, P_layers=2, alpha=0.8, att_heads='4,4', attn_dropout=0.2, batch_size=32, batch_size_target=100, beta_l=0.7, beta_t=0.3, concat_domain=False, concat_dropout=0.2, concat_stance=True, data_dir='./dataset/', device='cuda', dropout=0.2, emb_size=768, gnn_dims='192,192', hidden_size=768, leaky_alpha=0.2, learning_rate=2e-05, local_rank=0, max_epoch=15, max_seq_len=1000, measurement='cosine similarity', model_save_file='./save/1112_cm_pruned_situation2_4', num_target=0, num_train_lines=0, random_seed=4, sim_threshold=0.9, temperature=0.3, tk=5, tokenized_max_len=120, weight_threshold=0.3)
Done loading datasets.
Done constructing DataLoader. 
Done loading models. 
0th target dataset begin encode
1th target dataset begin encode
2th target dataset begin encode
3th target dataset begin encode
4th target dataset begin encode
5th target dataset begin encode
6th target dataset begin encode
7th target dataset begin encode
8th target dataset begin encode
9th target dataset begin encode
10th target dataset begin encode
11th target dataset begin encode
12th target dataset begin encode
13th target dataset begin encode
14th target dataset begin encode
15th target dataset begin encode
16th target dataset begin encode
17th target dataset begin encode
18th target dataset begin encode
19th target dataset begin encode
20th target dataset begin encode
21th target dataset begin encode
22th target dataset begin encode
23th target dataset begin encode
24th target dataset begin encode
25th target dataset begin encode
26th target dataset begin encode
27th target dataset begin encode
28th target dataset begin encode
29th target dataset begin encode
30th target dataset begin encode
Ending epoch 1
Training Accuracy: 58.951876019575856%
Evaluating on valid set:
favor f1: 75.63025210084035
against f1: 78.19548872180451
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 76.91287041132242
Best f1 has been updated as 0.7691287041132242
Evaluating on test set:
favor f1: 72.22222222222221
against f1: 72.82608695652173
Accuracy on 182 samples: 72.52747252747253%
f1 on 182 samples: 72.52415458937197
Ending epoch 2
Training Accuracy: 73.08319738988581%
Evaluating on valid set:
favor f1: 67.9245283018868
against f1: 76.71232876712328
Accuracy on 126 samples: 73.01587301587301%
f1 on 126 samples: 72.31842853450505
Evaluating on test set:
favor f1: 77.9874213836478
against f1: 82.92682926829268
Accuracy on 182 samples: 80.76923076923077%
f1 on 182 samples: 80.45712532597025
Ending epoch 3
Training Accuracy: 83.29934747145188%
Evaluating on valid set:
favor f1: 75.0
against f1: 74.19354838709677
Accuracy on 126 samples: 74.60317460317461%
f1 on 126 samples: 74.59677419354837
Evaluating on test set:
favor f1: 75.51020408163265
against f1: 71.42857142857142
Accuracy on 182 samples: 73.62637362637363%
f1 on 182 samples: 73.46938775510203
Ending epoch 4
Training Accuracy: 90.64029363784665%
Evaluating on valid set:
favor f1: 73.21428571428574
against f1: 78.57142857142858
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.89285714285717
Evaluating on test set:
favor f1: 70.37037037037037
against f1: 76.23762376237624
Accuracy on 182 samples: 73.62637362637363%
f1 on 182 samples: 73.3039970663733
Ending epoch 5
Training Accuracy: 95.0652528548124%
Evaluating on valid set:
favor f1: 74.78260869565217
against f1: 78.83211678832117
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 76.80736274198668
Evaluating on test set:
favor f1: 75.15151515151516
against f1: 79.3969849246231
Accuracy on 182 samples: 77.47252747252747%
f1 on 182 samples: 77.27425003806914
Ending epoch 6
Training Accuracy: 97.00244698205546%
Evaluating on valid set:
favor f1: 76.33587786259541
against f1: 74.38016528925621
Accuracy on 126 samples: 75.39682539682539%
f1 on 126 samples: 75.35802157592582
Evaluating on test set:
favor f1: 74.31693989071037
against f1: 74.03314917127072
Accuracy on 182 samples: 74.17582417582418%
f1 on 182 samples: 74.17504453099055
Ending epoch 7
Training Accuracy: 97.73654159869494%
Evaluating on valid set:
favor f1: 73.13432835820896
against f1: 69.49152542372882
Accuracy on 126 samples: 71.42857142857143%
f1 on 126 samples: 71.31292689096888
Evaluating on test set:
favor f1: 69.69696969696969
against f1: 63.85542168674698
Accuracy on 182 samples: 67.03296703296702%
f1 on 182 samples: 66.77619569185833
Ending epoch 8
Training Accuracy: 98.16476345840131%
Evaluating on valid set:
favor f1: 77.77777777777779
against f1: 77.77777777777779
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.77777777777779
Best f1 has been updated as 0.7777777777777778
Evaluating on test set:
favor f1: 71.03825136612022
against f1: 70.7182320441989
Accuracy on 182 samples: 70.87912087912088%
f1 on 182 samples: 70.87824170515957
Ending epoch 9
Training Accuracy: 98.53181076672104%
Evaluating on valid set:
favor f1: 80.32786885245902
against f1: 81.53846153846153
Accuracy on 126 samples: 80.95238095238095%
f1 on 126 samples: 80.93316519546028
Best f1 has been updated as 0.8093316519546028
Evaluating on test set:
favor f1: 73.03370786516852
against f1: 74.19354838709678
Accuracy on 182 samples: 73.62637362637363%
f1 on 182 samples: 73.61362812613265
Ending epoch 10
Training Accuracy: 98.77650897226754%
Evaluating on valid set:
favor f1: 72.22222222222221
against f1: 79.16666666666667
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.69444444444444
Evaluating on test set:
favor f1: 72.95597484276729
against f1: 79.02439024390243
Accuracy on 182 samples: 76.37362637362637%
f1 on 182 samples: 75.99018254333487
Ending epoch 11
Training Accuracy: 98.85807504078304%
Evaluating on valid set:
favor f1: 76.52173913043477
against f1: 80.2919708029197
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.40685496667724
Evaluating on test set:
favor f1: 75.90361445783134
against f1: 79.79797979797979
Accuracy on 182 samples: 78.02197802197803%
f1 on 182 samples: 77.85079712790557
Ending epoch 12
Training Accuracy: 98.91924959216966%
Evaluating on valid set:
favor f1: 70.47619047619047
against f1: 78.91156462585033
Accuracy on 126 samples: 75.39682539682539%
f1 on 126 samples: 74.6938775510204
Evaluating on test set:
favor f1: 72.15189873417721
against f1: 78.64077669902913
Accuracy on 182 samples: 75.82417582417582%
f1 on 182 samples: 75.39633771660317
Ending epoch 13
Training Accuracy: 98.81729200652529%
Evaluating on valid set:
favor f1: 77.68595041322315
against f1: 79.38931297709924
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.5376316951612
Evaluating on test set:
favor f1: 76.30057803468209
against f1: 78.53403141361255
Accuracy on 182 samples: 77.47252747252747%
f1 on 182 samples: 77.41730472414731
Ending epoch 14
Training Accuracy: 99.2047308319739%
Evaluating on valid set:
favor f1: 75.63025210084035
against f1: 78.19548872180451
Accuracy on 126 samples: 76.98412698412699%
f1 on 126 samples: 76.91287041132242
Evaluating on test set:
favor f1: 74.86033519553072
against f1: 75.67567567567566
Accuracy on 182 samples: 75.27472527472527%
f1 on 182 samples: 75.26800543560319
Ending epoch 15
Training Accuracy: 99.32707993474715%
Evaluating on valid set:
favor f1: 73.04347826086958
against f1: 77.37226277372262
Accuracy on 126 samples: 75.39682539682539%
f1 on 126 samples: 75.2078705172961
Evaluating on test set:
favor f1: 76.1904761904762
against f1: 79.59183673469387
Accuracy on 182 samples: 78.02197802197803%
f1 on 182 samples: 77.89115646258504
Best valid f1 is 0.8093316519546028
