Fine-tuning mBERT with options:
Namespace(D_bn=True, D_layers=2, P_bn=True, P_layers=2, alpha=0.8, att_heads='4', attn_dropout=0.2, batch_size=32, batch_size_target=100, beta_l=0.7, beta_t=0.3, concat_domain=False, concat_dropout=0.2, concat_stance=True, data_dir='./dataset/', device='cuda', dropout=0.2, emb_size=768, gnn_dims='192', hidden_size=768, leaky_alpha=0.2, learning_rate=2e-05, local_rank=0, max_epoch=15, max_seq_len=1000, measurement='cosine similarity', model_save_file='./save/1114_GAT1_tk10_mean_adjust_symm_contargetlang_3', num_target=0, num_train_lines=0, random_seed=3, sim_threshold=0.4, temperature=0.3, tk=10, tokenized_max_len=120, weight_threshold=0.3)
Done loading datasets.
Done constructing DataLoader. 
Done loading models. 
0th target dataset begin encode
1th target dataset begin encode
2th target dataset begin encode
3th target dataset begin encode
4th target dataset begin encode
5th target dataset begin encode
6th target dataset begin encode
7th target dataset begin encode
8th target dataset begin encode
9th target dataset begin encode
10th target dataset begin encode
11th target dataset begin encode
12th target dataset begin encode
13th target dataset begin encode
14th target dataset begin encode
15th target dataset begin encode
16th target dataset begin encode
17th target dataset begin encode
18th target dataset begin encode
19th target dataset begin encode
20th target dataset begin encode
21th target dataset begin encode
22th target dataset begin encode
23th target dataset begin encode
24th target dataset begin encode
25th target dataset begin encode
26th target dataset begin encode
27th target dataset begin encode
28th target dataset begin encode
29th target dataset begin encode
30th target dataset begin encode
Ending epoch 1
Training Accuracy: 58.15660685154975%
Evaluating on valid set:
favor f1: 73.75886524822695
against f1: 66.66666666666667
Accuracy on 126 samples: 70.63492063492063%
f1 on 126 samples: 70.2127659574468
Best f1 has been updated as 0.7021276595744681
Evaluating on test set:
favor f1: 70.53140096618358
against f1: 61.146496815286625
Accuracy on 182 samples: 66.48351648351648%
f1 on 182 samples: 65.8389488907351
Ending epoch 2
Training Accuracy: 73.12398042414355%
Evaluating on valid set:
favor f1: 70.47619047619047
against f1: 78.91156462585033
Accuracy on 126 samples: 75.39682539682539%
f1 on 126 samples: 74.6938775510204
Best f1 has been updated as 0.746938775510204
Evaluating on test set:
favor f1: 69.23076923076923
against f1: 76.92307692307693
Accuracy on 182 samples: 73.62637362637363%
f1 on 182 samples: 73.07692307692308
Ending epoch 3
Training Accuracy: 83.21778140293638%
Evaluating on valid set:
favor f1: 77.77777777777779
against f1: 77.77777777777779
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.77777777777779
Best f1 has been updated as 0.7777777777777778
Evaluating on test set:
favor f1: 71.50259067357513
against f1: 67.83625730994152
Accuracy on 182 samples: 69.78021978021978%
f1 on 182 samples: 69.66942399175832
Ending epoch 4
Training Accuracy: 90.68107667210441%
Evaluating on valid set:
favor f1: 75.67567567567568
against f1: 80.85106382978724
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.26336975273146
Best f1 has been updated as 0.7826336975273146
Evaluating on test set:
favor f1: 75.0
against f1: 76.59574468085107
Accuracy on 182 samples: 75.82417582417582%
f1 on 182 samples: 75.79787234042554
Ending epoch 5
Training Accuracy: 94.92251223491027%
Evaluating on valid set:
favor f1: 60.67415730337079
against f1: 78.52760736196319
Accuracy on 126 samples: 72.22222222222221%
f1 on 126 samples: 69.600882332667
Evaluating on test set:
favor f1: 55.88235294117647
against f1: 73.68421052631578
Accuracy on 182 samples: 67.03296703296702%
f1 on 182 samples: 64.78328173374612
Ending epoch 6
Training Accuracy: 96.6557911908646%
Evaluating on valid set:
favor f1: 77.31092436974791
against f1: 79.69924812030075
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.50508624502433
Best f1 has been updated as 0.7850508624502432
Evaluating on test set:
favor f1: 71.26436781609195
against f1: 73.6842105263158
Accuracy on 182 samples: 72.52747252747253%
f1 on 182 samples: 72.47428917120386
Ending epoch 7
Training Accuracy: 97.81810766721044%
Evaluating on valid set:
favor f1: 71.69811320754717
against f1: 79.45205479452055
Accuracy on 126 samples: 76.19047619047619%
f1 on 126 samples: 75.57508400103386
Evaluating on test set:
favor f1: 68.75
against f1: 75.49019607843137
Accuracy on 182 samples: 72.52747252747253%
f1 on 182 samples: 72.12009803921569
Ending epoch 8
Training Accuracy: 98.30750407830342%
Evaluating on valid set:
favor f1: 77.68595041322315
against f1: 79.38931297709924
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.5376316951612
Best f1 has been updated as 0.7853763169516119
Evaluating on test set:
favor f1: 73.86363636363636
against f1: 75.53191489361703
Accuracy on 182 samples: 74.72527472527473%
f1 on 182 samples: 74.6977756286267
Ending epoch 9
Training Accuracy: 98.22593800978792%
Evaluating on valid set:
favor f1: 76.1061946902655
against f1: 80.57553956834532
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.34086712930541
Evaluating on test set:
favor f1: 75.15151515151516
against f1: 79.3969849246231
Accuracy on 182 samples: 77.47252747252747%
f1 on 182 samples: 77.27425003806914
Ending epoch 10
Training Accuracy: 99.1231647634584%
Evaluating on valid set:
favor f1: 76.1061946902655
against f1: 80.57553956834532
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.34086712930541
Evaluating on test set:
favor f1: 74.25149700598803
against f1: 78.17258883248732
Accuracy on 182 samples: 76.37362637362637%
f1 on 182 samples: 76.21204291923767
Ending epoch 11
Training Accuracy: 99.02120717781403%
Evaluating on valid set:
favor f1: 78.18181818181819
against f1: 83.09859154929576
Accuracy on 126 samples: 80.95238095238095%
f1 on 126 samples: 80.64020486555698
Best f1 has been updated as 0.8064020486555697
Evaluating on test set:
favor f1: 74.07407407407409
against f1: 79.20792079207921
Accuracy on 182 samples: 76.92307692307693%
f1 on 182 samples: 76.64099743307663
Ending epoch 12
Training Accuracy: 98.4094616639478%
Evaluating on valid set:
favor f1: 76.52173913043477
against f1: 80.2919708029197
Accuracy on 126 samples: 78.57142857142857%
f1 on 126 samples: 78.40685496667724
Evaluating on test set:
favor f1: 72.72727272727272
against f1: 74.46808510638297
Accuracy on 182 samples: 73.62637362637363%
f1 on 182 samples: 73.59767891682785
Ending epoch 13
Training Accuracy: 99.2047308319739%
Evaluating on valid set:
favor f1: 77.87610619469028
against f1: 82.01438848920863
Accuracy on 126 samples: 80.15873015873017%
f1 on 126 samples: 79.94524734194945
Evaluating on test set:
favor f1: 71.00591715976333
against f1: 74.87179487179488
Accuracy on 182 samples: 73.07692307692307%
f1 on 182 samples: 72.9388560157791
Ending epoch 14
Training Accuracy: 99.38825448613377%
Evaluating on valid set:
favor f1: 73.58490566037736
against f1: 80.82191780821918
Accuracy on 126 samples: 77.77777777777779%
f1 on 126 samples: 77.20341173429827
Evaluating on test set:
favor f1: 70.44025157232704
against f1: 77.07317073170732
Accuracy on 182 samples: 74.17582417582418%
f1 on 182 samples: 73.75671115201719
Ending epoch 15
Training Accuracy: 99.10277324632953%
Evaluating on valid set:
favor f1: 67.9245283018868
against f1: 76.71232876712328
Accuracy on 126 samples: 73.01587301587301%
f1 on 126 samples: 72.31842853450505
Evaluating on test set:
favor f1: 68.75
against f1: 75.49019607843137
Accuracy on 182 samples: 72.52747252747253%
f1 on 182 samples: 72.12009803921569
Best valid f1 is 0.8064020486555697
